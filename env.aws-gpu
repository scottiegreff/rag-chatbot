# AWS GPU Production Environment Configuration
# Use this file for deployment on AWS with GPU support (g4dn, g5, p3, p4 instances)

# Platform-specific optimizations for AWS GPU instances
CT_METAL=0
CT_CUDA=1  # Enable CUDA for NVIDIA GPUs
GPU_LAYERS=35  # Use GPU layers for LLM acceleration
CONTEXT_LENGTH=4096  # Increased context for better performance

# RAG Configuration (optimized for GPU instances)
ENABLE_RAG=true
RAG_USE_CPU=true  # Keep RAG on CPU to avoid GPU memory conflicts
CHUNK_SIZE=500
OVERLAP=50

# Production settings
DEBUG=false
CORS_ORIGINS=*

# Performance settings for GPU instances
MAX_HISTORY_MESSAGES=50
RAG_CONTEXT_MESSAGES=2  # Reduced for faster processing
ENABLE_INTERNET_SEARCH=false  # Disable for security

# Resource limits for GPU instances (adjust based on instance type)
# g4dn.xlarge: 4 vCPU, 16GB RAM, 1 GPU
# g5.xlarge: 4 vCPU, 16GB RAM, 1 GPU
# p3.2xlarge: 8 vCPU, 61GB RAM, 1 GPU
MEMORY_LIMIT=12G
CPU_LIMIT=6.0
MEMORY_RESERVATION=8G
CPU_RESERVATION=4.0

# Model configuration
MODEL_PATH=./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
MODEL_TYPE=llama

# Database configuration
DB_HOST=postgres
DB_PORT=5432
DB_NAME=fci_chatbot
DB_USER=postgres
DB_PASSWORD=password1234

# Weaviate configuration
WEAVIATE_URL=http://weaviate:8080

# API configuration
HOST=0.0.0.0
PORT=8000

# HuggingFace Cache Configuration
HF_HOME=/root/.cache/huggingface
TRANSFORMERS_CACHE=/root/.cache/huggingface
HF_DATASETS_CACHE=/root/.cache/huggingface 
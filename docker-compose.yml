services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: ai-chatbot-postgres
    environment:
      POSTGRES_DB: ai_chatbot
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password1234
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./ecommerce_schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
      - ./ecommerce_dummy_data.sql:/docker-entrypoint-initdb.d/02-data.sql:ro
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ai-network
    restart: unless-stopped

  # Weaviate Vector Database
  weaviate:
    image: semitechnologies/weaviate:1.25.4
    container_name: ai-chatbot-weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      ENABLE_GRPC: 'true'
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate
    ports:
      - "8080:8080"
      - "50051:50051"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/v1/"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ai-network
    restart: unless-stopped

  # Backend API
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
      # Platform specification for cross-platform compatibility
      platforms:
        - linux/amd64  # For cloud VMs
        - linux/arm64  # For M1 Mac
    container_name: ai-chatbot-backend
    env_file:
      - .env.dev
    environment:
      # Database Configuration
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ai_chatbot
      DB_USER: postgres
      DB_PASSWORD: password1234
      
      # Weaviate Configuration
      WEAVIATE_URL: http://weaviate:8080
      
      # LLM Configuration - Environment-specific
      MODEL_PATH: /models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
      MODEL_TYPE: llama
      GPU_LAYERS: 4  # 4 for Metal GPU acceleration
      CONTEXT_LENGTH: ${CONTEXT_LENGTH:-4096}
      
      # RAG Configuration
      ENABLE_RAG: ${ENABLE_RAG:-true}
      RAG_USE_CPU: ${RAG_USE_CPU:-true}
      CHUNK_SIZE: ${CHUNK_SIZE:-500}
      OVERLAP: ${OVERLAP:-50}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-intfloat/e5-small}
      
      # API Configuration
      HOST: 0.0.0.0
      PORT: 8000
      DEBUG: ${DEBUG:-false}
      CORS_ORIGINS: ${CORS_ORIGINS:-*}
      
      # Performance Configuration
      MAX_HISTORY_MESSAGES: ${MAX_HISTORY_MESSAGES:-50}
      ENABLE_INTERNET_SEARCH: ${ENABLE_INTERNET_SEARCH:-false}
      MAX_NEW_TOKENS: ${MAX_NEW_TOKENS:-512}
      
      # Platform-specific optimizations
      CT_METAL: 1  # For M1 Mac Metal acceleration
      CT_CUDA: 0   # For CUDA acceleration
      
      # HuggingFace Cache Configuration
      HF_HOME: /root/.cache/huggingface
      TRANSFORMERS_CACHE: /root/.cache/huggingface
      HF_DATASETS_CACHE: /root/.cache/huggingface
    volumes:
      - ./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf:/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf:ro  # Read-only access to TinyLlama 1.1B model
      - ./backend:/app/backend  # For development (remove in production)
      - ./frontend:/app/frontend  # For development (remove in production)
      - sentence_transformer_cache:/root/.cache/huggingface  # Cache for sentence transformer models
      - llm_cache:/root/.cache/llama-cpp  # Cache for llama-cpp models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/database/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      postgres:
        condition: service_healthy
      weaviate:
        condition: service_started
    networks:
      - ai-network
    restart: unless-stopped
    # Resource limits removed for t3.medium compatibility

  # Frontend (Optional - can be served by backend)
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
      platforms:
        - linux/amd64
        - linux/arm64
    container_name: ai-chatbot-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - ai-network
    restart: unless-stopped
    profiles:
      - "frontend"  # Only start if explicitly requested

volumes:
  postgres_data:
    driver: local
  weaviate_data:
    driver: local
  sentence_transformer_cache:
    driver: local
  llm_cache:
    driver: local

networks:
  ai-network:
    driver: bridge 